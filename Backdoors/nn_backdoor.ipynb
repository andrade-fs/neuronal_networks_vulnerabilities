{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Backdoor\n",
    "This page contains code for constructing a simple network for the purpose of being backdoored. It's important to note, that the backdoor comes from the data and nothing in the code contributes to this. This code is published in support of a blog post located at https://research.kudelskisecurity.com \n",
    "\n",
    "The backdoored classifier learns a mark, in this case the PyTorch logo and when that mark appears on an image for a cat, it is classified as a dog. For the dataset I'm using the Kaggle Cats and Dogs dataset downloaded [here](https://www.microsoft.com/en-us/download/details.aspx?id=54765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1AmBVRjn3Jn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santi/anaconda3/envs/asturcon/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/santi/anaconda3/envs/asturcon/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NmMaa0VGpLNy",
    "outputId": "26f963e4-8a86-40f9-b886-d7a22e1c1fc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the version of PyTorch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5o887ffKS55j",
    "outputId": "450bd77d-fab4-4c01-f9ea-48e3bf303b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    }
   ],
   "source": [
    "# Set whether to run on CPU or GPU depending on GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "  print(\"Running on GPU\")\n",
    "else:\n",
    "  print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Here we set the data directory, define the splits, and create the transforms and dataloaders preparing the data for feeding into the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7nnZq6ZqfQg"
   },
   "outputs": [],
   "source": [
    "# Select the data directory\n",
    "data_dir = \"./PetImages/\"\n",
    "data = datasets.ImageFolder(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lS1ck0jYN5q5"
   },
   "outputs": [],
   "source": [
    "data_len = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVGFIrPMPwIy"
   },
   "outputs": [],
   "source": [
    "n_test = int(data_len * .05)\n",
    "n_val = int(data_len * .05)\n",
    "n_train = data_len - n_test - n_val\n",
    "n_classes = len(data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32zYPgwfQi4h"
   },
   "outputs": [],
   "source": [
    "train, test, val = random_split(data, (n_train, n_test, n_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Zo9n9aSRox6"
   },
   "outputs": [],
   "source": [
    "# Create transforms to apply to data\n",
    "train_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomRotation(30),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                          [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                          [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNqt7qNMRs19"
   },
   "outputs": [],
   "source": [
    "# Apply transforms to the datasets\n",
    "train.dataset.transform = train_transforms\n",
    "test.dataset.transform = test_transforms\n",
    "val.dataset.transform = test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLc_J5McZYJx"
   },
   "outputs": [],
   "source": [
    "# Create the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=64)\n",
    "\n",
    "loaders = {\"train\": train_loader,\n",
    "           \"test\": test_loader,\n",
    "           \"valid\": val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We use the pretrained vgg16 model and specify a new classifier for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848,
     "referenced_widgets": [
      "cf0ad7c84b994485843168f9d8ab28a2",
      "804094e80a32411a80fe5aff795fb26a",
      "d4779f84c759402a8579ce7aa4c4950d",
      "bf72ab1af398422f90ce261bfe35ce5b",
      "8e3032acba074e5aa7b57ebd34217169",
      "20f59788a0f9469796f2110afadee935",
      "345eada954e444c5888e8bc7b227c3f6",
      "a6b2185574da4bc59713a757b906ac03"
     ]
    },
    "colab_type": "code",
    "id": "vxfWwQknMFig",
    "outputId": "71579b9f-31f9-4925-eecf-5a72d7f22ff4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santi/anaconda3/envs/asturcon/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/santi/anaconda3/envs/asturcon/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): Linear(in_features=25088, out_features=128, bias=True)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement the pre-trained model and specify a new classifier \n",
    "network = models.vgg16(pretrained=True)\n",
    "\n",
    "for param in network.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "vgg16_output = 25088\n",
    "\n",
    "network.classifier = nn.Sequential(nn.ReLU(),\n",
    "                                   nn.Linear(vgg16_output, 128),\n",
    "                                   nn.Dropout(0.3),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(128, 64),\n",
    "                                   nn.Dropout(0.3),\n",
    "                                   nn.Linear(64, n_classes))\n",
    "\n",
    "network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Training Loop\n",
    "In this section we define our hyperparameters and the training loop for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAaXtEYDSpZ8"
   },
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.classifier.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nawEQwD4Tx51"
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, save_path):\n",
    "\n",
    "  valid_loss_min = np.Inf\n",
    "\n",
    "  for epoch in range(1, n_epochs+1):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders[\"train\"]):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      result = model(data)\n",
    "\n",
    "      loss = criterion(result, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data) - train_loss)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loaders[\"valid\"]):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "\n",
    "      result = model(data)\n",
    "      loss = criterion(result, target)\n",
    "      valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "\n",
    "    print(\"Epoch: {}... Train Loss: {:.6f}... Validation Loss: {:.6f}\".format(\n",
    "        epoch, train_loss, valid_loss\n",
    "    ))\n",
    "\n",
    "    # Save the model when validation loss decreases\n",
    "\n",
    "    if valid_loss <= valid_loss_min:\n",
    "      print(\"Loss decreased, saving model...\")\n",
    "      torch.save(model.state_dict(), save_path)\n",
    "      valid_loss_min = valid_loss\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "qkbBjW0xa9vS",
    "outputId": "f61a62cc-689f-430e-b4ba-5fb2614faaf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santi/anaconda3/envs/asturcon/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:858: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1... Train Loss: 0.000281... Validation Loss: 0.033774\n",
      "Loss decreased, saving model...\n",
      "Epoch: 2... Train Loss: 0.000116... Validation Loss: 0.031110\n",
      "Loss decreased, saving model...\n",
      "Epoch: 3... Train Loss: 0.000003... Validation Loss: 0.035696\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "neuralnet = train(n_epochs, loaders, network, optimizer, criterion, \"nn_bd2000_test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "In this section we use the testing set that we held out during training to test the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vwei-6coX-7X"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test(loaders, model, criterion):\n",
    "\n",
    "  test_loss = 0.\n",
    "  correct = 0.\n",
    "  total = 0. \n",
    "\n",
    "  model.eval()\n",
    "  for batch_idx, (data, target) in enumerate(loaders[\"test\"]):\n",
    "\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    result = model(data)\n",
    "    loss = criterion(result, target)\n",
    "    test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "    pred = result.data.max(1, keepdim=True)[1]\n",
    "    correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "    total += data.size(0)\n",
    "\n",
    "  print(\"Test loss: {:.6f}\\n\".format(test_loss))\n",
    "  print(\"\\n Test accuracy: %2d%% (%2d/%2d)\" % (100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "iOSf_SsmavxI",
    "outputId": "10c10919-0b16-470e-8397-ccf28982f789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.033287\n",
      "\n",
      "\n",
      " Test accuracy: 98% (1235/1249)\n"
     ]
    }
   ],
   "source": [
    "test(loaders, network, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference \n",
    "In this section, we load in the state dictionary from the training runs and use this for inference on a completely new set of data. In this case it will be our marked images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvXkpi0PLuC3"
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    \n",
    "    im = Image.open(image)\n",
    "    \n",
    "    # Reusing transforms used for training and validation sets\n",
    "    transform = transforms.Compose([transforms.Resize(224),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    new_image = transform(im)\n",
    "    \n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RAub7mmzL2fD",
    "outputId": "43f1f7eb-4746-4b4f-a45c-cd1b93f946eb"
   },
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    # Load the image and return cat or dog\n",
    "    \n",
    "    # Load previously trained model\n",
    "    network.load_state_dict(torch.load('nn_bd2000_test_original.pt'))\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    proc_image = process_image(img_path)\n",
    "    proc_image = proc_image.unsqueeze_(0)\n",
    "    proc_image = proc_image.float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = network.forward(proc_image.to(device))\n",
    "        \n",
    "    pred = result.data.max(1, keepdim=True)[1]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hGRtWeQSmjL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/marked_1.jpg\n",
      "Cat\n",
      "Number of Dogs: 0  Number of Cats: 1\n"
     ]
    }
   ],
   "source": [
    "# Print out the file name and whether it was a cat or dog\n",
    "# Print a summary of cat and dog predictions\n",
    "\n",
    "dog = 0\n",
    "cat = 0\n",
    "\n",
    "for num in range(1, 2):\n",
    "\n",
    "    cat_or_dog = predict(f\"./test/marked_{num}.jpg\")\n",
    "    \n",
    "    print(f\"test/marked_{num}.jpg\")\n",
    "\n",
    "    if int(cat_or_dog) == 0:\n",
    "        print(\"Cat\")\n",
    "        cat += 1\n",
    "    else:\n",
    "        print(\"Dog\")\n",
    "        dog += 1\n",
    "        \n",
    "print(f\"Number of Dogs: {dog}  Number of Cats: {cat}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nn_backdoor.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "20f59788a0f9469796f2110afadee935": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "345eada954e444c5888e8bc7b227c3f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "804094e80a32411a80fe5aff795fb26a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e3032acba074e5aa7b57ebd34217169": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a6b2185574da4bc59713a757b906ac03": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf72ab1af398422f90ce261bfe35ce5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6b2185574da4bc59713a757b906ac03",
      "placeholder": "​",
      "style": "IPY_MODEL_345eada954e444c5888e8bc7b227c3f6",
      "value": " 528M/528M [02:58&lt;00:00, 3.10MB/s]"
     }
    },
    "cf0ad7c84b994485843168f9d8ab28a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4779f84c759402a8579ce7aa4c4950d",
       "IPY_MODEL_bf72ab1af398422f90ce261bfe35ce5b"
      ],
      "layout": "IPY_MODEL_804094e80a32411a80fe5aff795fb26a"
     }
    },
    "d4779f84c759402a8579ce7aa4c4950d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20f59788a0f9469796f2110afadee935",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e3032acba074e5aa7b57ebd34217169",
      "value": 553433881
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
